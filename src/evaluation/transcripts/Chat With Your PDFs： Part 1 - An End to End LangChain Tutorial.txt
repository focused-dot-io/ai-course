Hey, y'all. I'm Austin Vance. I'm the CEO and cofounder of Focus Blabs. I wanted to do a quick demo on how to do a rag that works with your PDF. I wanna make this demo, like, full end to end. And so what we'll do is we'll start with, like, document import, chunking, management, storage, and then we'll go through building the LLM. We'll use lang serve, to serve web requests. And then finally, we'll build out a React front end with TypeScript that gives you a nice little chatbot. I depending on length, we'll split this video up into three or four. We could be one shot or it could be three ish sections would be my guess. Let me know if you have any feedback. Like and subscribe, you know, like, just let me know how you like it. So thank you very much. So right here, open just to start is just a totally empty project. So this is a really easy kind of follow me along style project. And so when I have a totally empty project, I just spun it up with Poetry. I like Poetry a bit more than pip. What we can do is just start by installing the lang train CLI. And if we grab the lang train CLI, it gives us everything we need to start a LangServe project and we'll install everything from scratch. This is very follow along. So let's just do, you know, lang chain poetry run lang chain app newdot. So that'll just give us a brand new lang serve app straight in our, in the base of our directory, which is great. So I'm gonna just start with this. And we have our initial commit to work off of. So when we start with a new Langserve project, what ends up happening is what ends up happening is we get a couple of things. We get packages and we get app. We'll just focus on the app section for now. And in app, we have a server. We don't need to use anything here yet, but what I wanna do instead is just create a new package so we can start to import, start to import documents. So our first import, we'll just name it importer. Maybe seems fairly simple. And I wanna grab some source documents. So I'm gonna actually just grab them from, the main, you know, some from another project. So these source documents are a handful of court case documents that came out of the Epic Games versus Apple trust lawsuit. So let's build a rag that can let us ask questions about what happened between Epic and Apple. So I now have my document importer package, and I wanna just create a new file, and I'll call it, like, load and process. Load and process py. This is the the file that's gonna load all of our documents and then process them for, for the LLM. And so what we'll need to do is there's a few steps. So what we do is we need to parse the PDFs since these are not, like, in text format. We need to get them into a text format and then chunk them. So chunk them means splitting them up into, you know, pieces that then our vector store can store semantic similarities on. And then from those pieces, we need to store them in a vector store. After we chunk them, we need to embed and then chunk those and or sorry, embed those chunks and then store them in a vector store. I am going to use for this project the lang chain Postgres vector store, which I think is just an awesome way to get started. It's on every single major cloud provider. There's a really simple Docker pack or a Docker container you can pull down. It's great to use and it just gets you going in no time. So there's a couple other things we'll need to do just to get started. I'm gonna grab an m file and just paste that in. That m file we won't look at. So we have I just pasted in an m file just kind of in the base of this directory here, and that m file will let us then work off of, work with OpenAI and everything else. So let's start with our importer. So our importer, I want to use the directory loader from langchain. The directory loader is, is like designed to just grab a whole bunch of stuff right out of a directory. So we'll grab our directory loader and we'll load from the OS path. Let's import this and we'll grab, you know, path and we'll just go up one to source dot source docs, right? So that's pretty simple. There's a few other things we want to give to our directory loader. So let's tell it to grab all the PDFs and I have Copilot that's being pretty helpful right now, sometimes it's not. I like to use multi threading and we'll show our progress and we'll have, some concurrency. And then I wanna use the unstructured PDF loader. And as you can see, for me at least, Copilot just suggested it all the way through, which is really, really nice. And for our path, let's do, absolute path, actually. So we get there. That was that that warning was. So now we have our loader. This loader, although formatted funkily, will allow us to then grab all of our documents. If you watched my video on parsing, transcripts out of YouTube, what we do is we are now loading all this class will let us load all the text out of a PDF or a set of PDF documents using unstructured. Great. So the simple thing to do now is just do docs dot, equals loader dot load. And if we do that, we run it. If we print our docs, you'll get them. It'll be rather slow. So what I wanna do is sample size one just so we can kinda see it, and we'll run our thing, and we'll see what happens. We're gonna start to get some errors right off the bat. And that's one of the things I wanna show you is we can walk through these errors really kinda step by step, and it tells you exactly what we need. So when I ran that, you saw that we have a missing package. So let's just install that. So Poetry add our first package here, and then that'll pull that in and just keep running through. And we'll let the errors of the code kind of guide us on what's next. So we already got our loader, and if you notice, we didn't get anything out of it, which is a little surprising. This is a pretty common error we get when we use concurrency. And so, it swallows errors. So if I turn concurrency off, you notice we have our unstructured package missing. So I really like turning that off while I'm developing, making sure I get one, and then continuing. So if I just poetry add unstructured, and I do all the documents, if you look at their their package, we can grab kinda everything. And this is an error with generating our brand new project as we need to change our Python version. So just kind of following along, exactly what you would do is you'd pop into your project toml and you would go to your Python version and you would make it match what would work here. So what was happening here is this said we could go all the way up to Python version 12, and we don't we can't do that for unstructured. So now if we pull in unstructured, we have our Python version matching. Unstructured will be able to download everything it needs, and then our load and process, should give us our first document. We'll let everything install really quickly. And while it's installing, Where did I put that? Oh, I was working in a net. So let's just copy all this over to my loading process, and we'll work out of here. That was a mistake. So still just finishing installing. Just need this last one. You don't have to do all documents. You could just pass in PDF or PPTX or whatever document you want when you're installing unstructured. I do all as like, the project it doesn't blow up the project too much, and it helps me and I just keep iterating. So now if I do this, we have our sample size of one, and we should print out our first stock. And so I run that. It's gonna take a second, it looks like, which is a bit of a bummer while we all wait. So while we wait, let's keep going. So the next the next part of this is we want to do to create some embeddings. So OpenAI, gives us the easiest way to do some embeddings. And so we're not going to do it this quite this way, but we are going to take here and we're going to pass in a model, and we'll grab our embedding model. Our embedding model comes from config. And if we look at, you know, OpenAI offers one pretty simple embedding model. And I like storing config kind of all in one config file. You can put it in your in your app or a package or wherever you want. I tend to put it at the top and just, you know, create a Python file named config. And then I could put something like our embedded model, which is our, you know, default embedding model for OpenAI. And if you notice here, here's all the text that was in that first document parsed out. It's in a we printed it out. It's in a document class or instance of a document. And then we have the page content, you know, with the the parsed out con parsed out content. We're not doing much, like, cleanup on this, or I don't wanna do that in this one, but we can look we also have some metadata that unstructured and langchain add for us, including a source, which points to the file on my computer that that is in. So cool. So now we have that loading and we have our OpenAI loading our embeddings. We'll import both of those. We can pop over now, to our, splitter. So when you have a big document or a lot of documents, what ends up happening is they're so big the LLM can't process. When you do a similarity search, you might find something that has the right, like, context, but then you pull in all this context, and then it's really hard for the l m LLM to extract meaning. And so then your chunk is essentially too big. So we wanna split up our documents into smaller chunks. If instead we make our chunks too small, what can happen is our our vectors or our our vectors and our, you know, in our vector database, those won't hold enough context to give to the LLM. So we'll pull in maybe like one sentence or part of a sentence or something like that. And so the LLM can't you know, take what was returned from the document and then start to answer questions. So the kind of cool thing right now is the semantic chunker. So we're gonna use that. We're gonna use a semantic chunker. And for a semantic chunker, we're gonna use we're going to use our OpenAI and Better to do it. And so how the semantic chunker works just kind of really quickly is it passes it splits all of your document up into sentences and then looks for similar sentences and then groups those together inside of your vector store so that, so that it's faster, easier to split, and you have more context inside of each chunk. It's a really nice way to make sure that you get really clean splits on documents that are, long form text. So they don't work well on things like they don't necessarily work that well on things like lists or code or anything like that, of course, but works really well on text and especially on legal documents. So, the semantic chunker isn't here. And so we need to grab it from, you know, Copilot is actually suggesting it for us. So let's grab it. So we're going to download the lang chain, experimental package, which will include our semantic chunker, and now we can import that. So our semantic chunker, we can name it like text splitter or something like that. I see that a lot in the OpenAI documents. We name it text splitter, and then we wanna split up our thing. And really nice, Copilot continues to just give me a ton of great suggestions. We have all of our chunks. So let's print out our our chunks now. So if I go here and, print out our chunks and I rerun this, it'll take a second to run. So we'll keep working while it's running, and we can look at it afterwards. Oh, we get an error. And so we're getting an error because we're using the OpenAI. We're using OpenAI's, embedding model and we need to have access to our API for that. So in order to do that, we can do one of two things. We can import our environment variables, which includes the OpenAI API key in the correct name, which is, you know, if you use this environment variable, then we end up being like it just happens automatically or we can pass that API key in explicitly. So to do that, I like to use just Python. Env and I use load dot end for that, but we need to install it. So, we'll do poetry and Python, dot m. And there it is. So now we can come up here and import that and run again. So that'll grab now that, dot end file I pasted in earlier. And if you notice, now we're missing OpenAI. So we're just letting our errors continue to guide us. So let's poetry add OpenAI. And that's there. So let's run this again. And if we look, there's actually an issue here. We're gonna get a deprecation warning. Oh, first we need to do TicToking. So just continuing to work through poetry ad Tic token, which is the splitter that OpenAI uses, and we'll just keep running this and continue to get our errors or we'll get a deprecation warning. So here's our deprecation warning. And actually, if you look now, we have our document and it should be split up. So if you notice, there's a document here and there's a document here. So we have a few documents now that we're working with. Another one starts. So we have one there, one there, one there. You know, we could print out the length, but we're seeing that we're getting a lot of documents now. That's just text. So here's another one with different page content, different metadata, that kind of stuff. So that works really nice. We're seeing each chunk now. I want to handle this deprecation warning really quickly because I don't like that stuff making everything noisy. So let's grab our, this and install our langchain open AI package. So, langchain has decided to move OpenAI into its own package. And so now we can grab this from not the community package, but instead from the OpenAI package, and we'll get a no deprecation warning. So now we have our chunks and we need to have something to do with them. So what we do with those chunks is fairly important. You can store them in any number of vector databases. I can do tutorials for them if you want. But, you know, there's Redis, there's Pinecone. We're going to use PgVector in this tutorial. There's a ton of others that are, you know, run as SaaS products. I like PgVector because it's just installable, runnable and feels great. So, to get PG vector going, there's a, there's a there's a Dockerfile, which is, you know, PG vector here, and I'll show you how to use that. So we'll kill it, and we'll do the whole thing from scratch. So Docker kill just you don't have to do this if you don't haven't pulled it before, but I was running it in the past. And so we'll kill that. And if we run, we're gonna run RPG vector. And I just for local, I'd pass in no auth. You know, it's fairly simple to do and you just keep going. So not necessarily the best practice, but it's easy enough for local development. And then when you're developing in prod, you would want to use a full vector database or you would use a full username and password and all that kind of stuff. So, I'm gonna run PG vector in the background. Let's, remove that because I already had that name, and now it's just running. So if I do docker ps, I have pg vector up and running. It's exporting the right ports. Make sure when you run it, you say, like, expose, you know, port five four three two to the host machine and this is the correct container. And then, you're good to go. So now we have dot pg vector running and we can start to work with it. Pg vector is an extension to Postgres. So depending on your cloud provider, it might not be installed, but it is installed on AWS and dot, DigitalOcean and cloud or GKE or Google Cloud, sorry, and, Azure's, you know, default managed Postgres services. So it's pretty much everywhere and you can use it. And it's really great for local development because it's fast, it's easy, and it can store a ton. So, let's just start using it. P g vector dot from from documents. And so let's grab our p g vector. And so we're going to pass our docs in, and those docs are the chunks that we had just created. So I pass those in and then, we need to give that. Sorry. So we look at the arguments here. We pass in documents, our chunks. And then what else do we need to give it? We need to give it embeddings. So we need to name this guy. Beddings. And let's see here. Why are we getting upset? Oh, spelling error. Cool. So, you also need to pass in a collection name. You have to use this both for retrieval and for for retrieval and for storage. And so we need to use the same collection name. I often put this into config. So if we look at our config, I can add, you know, PG, collection name, and let's just name it, like, PDF Rag. And so now we can pull this in just so it's now in one spot. We can say, you know, PDF Rag. Or not PDF rag. We'll call it what did we name this over here? PG Collection Name. Cool. And so now we have that, and we need to pass in a collect connection string as well. We have that in our environment variable in our environment file, but you could also just pass in you can also pass it in for the local development without any username or password. It's really simple. Our username is gonna just be Postgres. It would look something like this. Sorry, in quotes. So it looks something like, you know, Postgres. We don't need a password, actually. This is just default and then local host five four three two. And then we need to give it a database name. So we'll name our database like PDF rag, vectors. Vectors. I just like vectors more than vector store, and that would give us everything we need there. And then, I tend to pre delete the collection just so when we, continue to iterate, we will see the we'd see the collection, you know, cleaned out so we don't just double everything up over and over again. So if I run this, we're gonna start to get, you know, missing package errors all over again, and we'll let that run for a second. Boom. And if we look here, we have no, you know, Psycho PG, package. So poetry add psycho PG, and we just kind of walk our way through missing dependencies again. And we run another import and we let that run. It kicks over. It gets embedded, and we run into an error. Let's see what's going on here. So we are missing our database, so that's really good. So it's telling us exactly what we need to do. So we hop in and we can just do psql, and connect to our local database. And then we would just do create database, whatever I named it. PDF rag vectors. So now that is there, so we can exit back out of that, pop in here and rerun. And as that runs, you'll notice we are missing PG vector. So that's a Python package that helps with the embeddings. So we just continue to work our way through missing packages. Add pg vector, and run again. Boom. The collection not found is an error that comes up if we haven't created the collection inside of our vector yet or inside of that database yet, but now we're good to go. And we can actually see that if we reconnect to our database and we jump into our PDF rag vectors database and we do a describe. Langchain created two tables for us, the Langchain PG collection and the Langchain PG embeddings. So if we look at these, we can describe each of these and we'll see that there's, you know, an index, we have a name, some metadata, and a UUID, and then if we look at the other one, which was, we're getting really quickly embeddings, you'll see that it has a primary key or sorry, a foreign key linking the, linking the collection with each embedding. So that's how you can put, you know, more than one set of collections or different types of documents or chat bots or something like that into one into one table. So, and if you notice, langchain pgembeddings, we'll get five rows. And if we print out the number of chunks that were here, we go back and look at that. There were five chunks That shows we've imported all of our vectors for at least that. So while, we go work on the LLM, I'm going to start importing all the other documents. So I'm gonna just run this. That'll get going locally. And now I'm gonna hop up into my app, and I wanna create a new file. And I'm gonna call this, you know, my rag chain. And a rag chain, I don't know. There's probably a better name for it, but simple enough. And we'll watch this kind of chug along while we start working on our, chain. So for this, I wanna use langchain's, l c e l or l a cell, I think is how they want you to say it. But use lang chains LaSalle to build out a chain that allows us to then start querying that, vector store. So we could do this with an agent, but it's a it's much easier to use retriever. And so we can use pgVector as retriever, but let's not go there quite yet, although we can pretty quickly. We'll use PG vector as retriever. We'll just embed those documents straight into the prompt rather than using, like, an agent to go and search. I'll do a tool or I'll do a tutorial later on doing an agent style, retrieval of a document store or a vector store. So there's a couple of things that are super important. The first thing is we just wanna use the, we just wanna create our, like, basic, you know, template. And so there's a really base kind of there's a base prompt that we give to any rag, and we can always make this much more complex. You know, give it a personality, tell it what to do, but let's start with this really basic, you know, RAG RAG prompt. And so when you have a RAG prompt like this, we need to give it both context and a question. The question is whatever our user is gonna provide, you know, tell me how app why, Epic sued Apple or what was the outcome of the case. And the context will be the documents returned by the vector store, similarity search. So when I do that, I then need to, compile that template into a prompt. And so, Langchain makes that super easy. So now I have an an answer prompt. And actually, this is already a runnable. And so I can come in, really simply, I'll get an error, but I could come in and just, like, start querying that, because l c e l passes kind of each one of these things as a runnable. So it's not that interesting to do it that way, but we could start there and say, hey, why don't we grab an LLM and start playing with this? So so if we use chat open AI as the LLM, and we'll import it from the new one, and we will say, you know, temperature zero. That kind of brings down the randomness in the rag. And then we, grab our model, which will use GPT four. And we wanna stream because, streaming is expected in any, LLM app now. So we can call this our LLM. And that feels really nice. So really simply, if we go down here, we can say we have a final chain and that can immediately be our answer prompt, piped into our LN now, and then we would pipe that into a string like maybe a string output parser. And there you go. Boom. We now have a l c e l chain. We could call it by saying something like final chain dot invoke, and we could pass in context. Context could be something as simple as nothing. And a question could be what is the meaning of life? Jeez. Jeez, Louise. Copilot is not helping me right now. There we go. And if I print this, and I print this, we will get our results. And so there is our missing key again. So let's just do load dot end and import that. And, unexpected keyword streaming. So where did I do that wrong? Chat open AI streaming. Not what I expected. Just remove that for now. And we invoke our prompts. Oh, I think I did the wrong one. No. I didn't. Cool. And it should have been rather fast. So I'm a little curious on where it is stuck. Oh, there it is. Because it wasn't streaming tokens. That's why. So if you look here now, we called our LLM. We passed in a context. There was nothing to do, nothing there. And you can print that out. We have some other videos where you can, like, start to inspect these things. You can also, if you have LangSmith access, look at these runs straight inside of an LangSmith and see what was passed in and how the prompt was compiled. But, this isn't very helpful because what we need to do is be able to get our final or get our documents into where our, context is. So let's make this question a little more meaningful. Let's say why did Apple or did Epic sue Apple? And so I that should give us a slightly different response. This might be common enough knowledge that the LLM can answer it without the documents, but we'll get better responses as the documents get, added in. So, I'm a little surprised about the streaming. We'll try that again with streaming. True. There. So here Epic Games did Apple Prime in 2020. So this is, you know, nothing. And the other thing that we're not getting is we're not getting any, we're not getting any documents or sources or anything like that. Or you could say what was, but we'll go from here. Let's run that again with streaming and see what happens. And it seems to have worked. I must have spelled it wrong before, and I didn't notice. So what what do we need to do? We need to do retrieval. So retrieval is when you grab the context out of a vector store, by passing your question in an embedded version of your question in and LCEL and Langtry make that super duper easy. So where do we start? We start with p g vector again. So we just take our p g vector, and we import it. I like to do that because I get better completions. And then I can pass in my collection name. My collection name is the PG collection name just like before. We'll need to import that, and we'll need a connection string. Just like before, we can import that, and then we will need our embedding function one more time, And our embedding function allows the retriever to embed our question. And so we just pass that straight in. The defaults are fine there, and it will find everything just great. So now we have PG vector, which we can call our vector store. So we now have our vector store, which does just what I said. It allows us to connect to PG vector, grab that, grab all of our similar chunks and come out of it. So how do we use that? That's a really good question. So to use our vector store, all we wanna do is pass our, pass our question in to our vector store as a retriever. So if I look at how we do that, we can create a runnable. So I want to, what I want to do is just pass in a retriever here. So if I go here and I say vector store dot as retriever, I now have a retriever I can pass in and that retriever now no longer requires the context. It only requires the question, and we should be able to invoke. Nope. Database vectors does not exist, so I'm going to need to change my end file, which I will have to cut out or I can pause really quickly. Alright. So if I fixed my database from the environment, what we get now is another error, and it is an error around the invoke. So, I think it's expecting just a full string rather than a question key. So let's do that and pass it in. And so now we're saying we're expecting the retriever to have returned a list or got a list and it was expecting, you know, some sort of chat prompt template. And so now what we need to do is take our retriever, and combine all those documents into something that make it, much more or or handleable by our context. So how we do that is we pass the documents that come out of our retriever, and that will return the key documents. And you can actually see that. Let me show you how you see that really quickly. So if I comment all that out and I just run this or I delete all of that, if you'll notice, it's returned a bunch of documents, as a list here. And so that comes in as a pure list. So now what I want to do is I actually want to take all of these and I could just say context is this. And now I'm passing my context as what is coming from that retriever, and it expected context and question. And so we need to pass our question through. So now we need to do question. And now we can pass our question through And now we can say go back to what we had which was expected string or buffer. So we need to pass into there our question. So and so now if we pipe our question into the vector store and we also pass our question through to the next thing and we run this one more time, you'll notice we feel like it's actually running this time. And, Epic Games sued Apple because they disagreed with Apple's policies and procedures. And so now we actually have a more succinct and backed by context, output. And what I would love to do is start to see how we see documents coming out of this and all this other stuff, and we can always, of course, stream. So if we do, like, four chunk or c n final final chain invoke, Brent, c. We'll get, each part of the c. S t r e a m. Having trouble typing tonight. And you'll see that we can actually stream parts of this out. Epic Games suit Apple. Cool. And so stream, logs, which is what I really like. And this is async, so you have to do an await. We'll do that here. And we have to wrap all of that in a thing. So a saying for and then we await the if we run this, of course, it won't finish. So we need to put that into, main. So we would just say that main main. And we can I think and then we can use async Iota run our main? And I'm just showing you this. And now what's nice is we can actually see here I wanted to show as we get our trap prompt, but even more so we get our documents. And so when I do this async run, and this will actually be really useful later, is it actually returns all the documents that matched in our final run. It returns all the documents that matched our similarity score so we can actually source the responses, and give citations just like in ChatChiBT or something else. So, let's delete this for an hour. We'll keep it there. We're going to not use that. We'll pull it out of this later when we start using it inside of Langserve. So this is actually really all you need to start using lang serve. There's a ton of stuff we can add. There's no memory. There's, ways to make the rag more productive, like multi question, that kind of stuff. Let's jump into that in the next video. So, to round this video out, I'm gonna pop into our server, and I'm gonna just drop in our final chain. And I'll just and this is how easy lang serve is. I'll drop in our final chain that imports right from our other one, and I'm going to just run this. So now already up and running is our, is our web server. And if you look here, the web server up and running has the rag output or has the rag endpoints. We have it, you know, invoke, batch, stream, log. They all work straight out of the box. It's using type inference, using pedantic to know what you need to do. And then really nice is each one of these endpoints, you can just go to slash rag slash playground. And now all of a sudden we have our input. So I can ask, oh, we need to give it a type, I guess. As pedantic wasn't able to infer, what's going on with our chain probably because of all of this funkiness. We need to give it a little bit more information on, the type that it should be, doing. So in order to do that, it's really simple. You would just create a class inside of here, and it's gonna use, you know, a pedantic type. So you'll just say class, you know, and you could say like, I don't know. What do we wanna call it? We can call it rag, input. And that's gonna just take a typed dict, dict. And, we're not gonna give it chat history, although it wanted to complete it that way. We're just gonna give it a string. And then our final chain here, we can wrap, dot width types and then pass in our rag input. Now this is getting pretty unwieldy and long, so let's, start just breaking it down a little bit. We'll make this bigger so you can see what I'm doing. So if we do kind of break it down into each of the individual components, you can see a little nicely more nicely what's going on. And I don't know why we auto completed that. And that should be, oh, this needs an argument. If we look here, it takes an input type named argument. We'll name it the rack input. And so now, because lang lang serve, I didn't run it the right way and I'll show you how to run it appropriately. It didn't reload. So let's kill this server. There's two ways to run it. One is you can run it from the command line and you can just say, poetry run lang chain, lang chain serve. That'll start it and it starts with a reloader or you can run it using, Unicorn or NuvaCore and pass in the dash dash reload. So now any time I reload my code, stuff will reload. And now because I passed in, the types, we know we need a question. And so I can say why did epic sue apple? And if I, send this and our output here, we immediately get exactly what we'd expect. Epic Games suit Apple because they disagreed. And if we look at our intermediate steps, we actually get our documents here and you can see the responses. We get our question. Here's our context. Context has, you know, document here, a document here, a document here. And so on our front end, in my next video, what we can do is we can parse those documents out and add sources. You can actually even download the the source, straight straight for yourself and read and review what the LLM is outputting. So really quickly, what we did today is we went back through to just overview what we did is we were able to scan an entire directory of PDFs, you know, using our unstructured PDF loader and the directory loader. We were able to make that concurrent and so rather fast. We embed those, we split and chunk those documents into reasonable sizes using the new and cool semantic chunker in langchains experimental, and then we store those documents inside of our Postgres database locally running in Docker. Then we set up a simple RAG, chain in langchain. So we take a simple template, that just says answer the following question. We retrieve our documents from our document store. We retrieve our documents from our document store. We then pass those into our answer prompt. We then give all of that information to our language model, so GPT four, and then we push that out. We then load that really, really easily into a whole set of endpoints for not there. Here. We load that into a whole set of endpoints for lang serve, and we end up with a fully functioning, working back end that supports streaming and has a PDF rag right out of the gate. Thank you very much. And I will do part two on Thursday where we will build the front end for this. The front end we'll do in TypeScript with React. We'll handle streaming. We'll get the documents out. And then I'll also add to the end of the video, or actually, maybe we start with this. We'll make this rag chain slightly more complex and feature rich. We'll add things like multi query, so we'll use an LLM to parse and rephrase the question so we get better similarity searching or maybe a more more breadth in our searches. And then we'll also add chat history. And the nice thing is when we add chat history, we can use Postgres as well. So we only need one database to support the entire thing. Talk to you guys on Thursday.
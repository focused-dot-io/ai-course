Hey, Austin Vance, CEO and cofounder of Focus Labs here. I wanted to do a video today that wraps up the PDF end to end RAG tutorial code with me style video. I I want to just like hop right into code and get to it. This video is really going to focus on adding more sophistication into our RAG and covering some of the gotchas that you get with the standard RAGs and showing you how easy it is to modify a LaSalle pipeline. Diving straight in. If you remember in the first part, we put together a really simple rag. We loaded all of our context using unstructured. And so load our context using unstructured, shove it into PG vector, use a LaSalle line chain chain to get that pipeline up and running. This is just a simple LaSalle chain. So we have everything that we need here. We use PG vector, we retrieve from our vector store and we're good to go. And then we have our server serving with lang serve. In Part two, we put together our front end. Our front end is just a single React component that chats and then we deployed it to DigitalOcean in Part three. So let's get this thing running locally. So if I just start my front end and say npm start and then I pop into maybe start the back end. And if you remember, we have to have Docker running. So if I do Docker ES, we have Docker running here. You can use a Docker command that looks something like this. So this is running the PG vector official image using the local data so we don't lose it when the container starts or the computer restarts and then we just don't require username and password. So we have Docker running our Postgres, we have our back end running and our front end running. So if I pop in here, I can say what is the case about And really quickly, we get our retrieved documents along with a summary or an answer to our question using the content of those retrieved documents. So there's two things I really wanna do in this. One is right now, there's no history. So I can say, can you who you tell me a bit more? And I get this based on the context provided. It doesn't really understand what's going on. There's not history to say, oh, based on what was here, we can continue a conversation. And that's expected out of most chat applications. They build on the conversation like a chat versus each question being executed in isolation without any knowledge of what's there before. And then the other thing is rags, one of the ways rags work is they use semantic similarity to search through your documents and then decide what parts or chunks of these documents that are stored in your vector store are similar to the question. When I do this, it's looking for words like case, and then it's trying to figure out what it's about. It's using it uses some language modeling to say, okay. There's a lot of other words or phrases that could ask what the case is about. So we'll use a we'll use that to figure out what chunks are most reasonable, and that's our embedding step that we've done before. What can happen is wording matters a lot. And so if the words aren't right, then what will happen is we won't find the right documents or the right chunks in the right documents, and so our answers will be wrong, and that will lead the LLM to hallucinate. So what I really wanna do is start to do something like multi query. So what multi query does is it uses the LLM to ask the question in a handful of ways. And so what that'll do is say rather than what is the case about, we could say something like, tell me the details of this case. What is this lawsuit about? Whatever it is. And that will end up getting a breadth a larger breadth of documents, which will give more context to the LLM, which will allow for hopefully more accurate answers. And to do this, I really want to demo LangSmith. So if I pop into LangSmith here and I log in, and we can pop into LanghtSmith here and see we have a project, our PDF RAG project, and I'm going to just drop this into our N file so we can start to see all this stuff work and then I'll grab a new API key for this demo specifically. If I pop into API keys and I say create a new API key, copy this guy and drop it in here, we now have LanghtSmith tracing going on here. So I'll show you this really quickly so you can see what it looks like to start. So we'll just say in, like, the last hour. And if I run my rerun my back end, sorry, if I rerun my back end so we get those environment variables loaded in and I pop back into the front end here and I refresh and I say, what is the case about? And so we get all of this, but even better, we pop in here and you can see our rag is already starting to for some reason, it restarted the it paused the context. So I wanna go see why it did that. We can look in the front end really quickly. It should not do that based on yeah. Let's when we pass this open when hidden true into our touch event source, it won't pause the context when we lose visibility. So now if we come here, this one will never finish, but this one will because this never finished. You can see where it died here. And if we pop in here, you can see what was the case about. You can actually see the retriever is retrieving a certain set of documents. You can see the content of those doc or the source of those documents here, and then you can see then you can see the output of the full run. And pretty cool, actually. So if we click on OpenAI, we can see what was sent in and we end up with a really nice, really nice trace that allows us some introspection. And I think that'll be really helpful as we dive into this video. So let's do multi query first because I think that one's really actually pretty simple. So if we go down to our retriever and we look here and we see Vector Store as Retriever and we just call this MultiQuery and then we pop up here and we say VectorStoreAsRetriever and MultiQuery equals that, we just end up with the exact same thing we had before. So it's not that interesting. But then what we actually want to do is we want to say Multi query retriever and then we pass in our vector store and we're going to actually not pass in the vector store. We're going to say from llm import that and we're gonna pass in retriever, and rather than vector store here, we're gonna use this, and we're gonna say LLM is gonna equal LLM. And our LLM for context up here is just OpenAI with GPT-four Turbo. Blank serve already restarted for us. So now if I come in here and I refresh and I say, what is the case about? I'll get an answer. We're getting a lot output. And actually, if I pop over here, let that continue to run and looking at this, I think this is really cool. So if you look here, I blew the window up so you can really see. But we have now a retriever, which is our multi query retriever. And if I zoom out a little bit, so this is a little easier for everybody to see. You'll see what is the case about gets asked here. And then we answer with you are an AI module module. Your task is to answer with three questions that rephrase the original question and it says you can describe can you describe the details and subject matter of the legal case? So we get better breadth. And then it runs through each of these and we get new documents. Boom. Boom. So we get more documents that get hydrated into our context. And then we pop in and we actually see that getting run into OpenAI here. So we see all of our documents being handed to OpenAI right there. That's it for the multi query retriever. It's actually that simple to drop it into a RAG. Now, chat history gets a little more complicated. Let's do that next. Langchain provides a really nice convenience class or runnable called runnable with message history. So if I create a runnable with message history and this thing takes a handful of parameters. So I'm going to drop this down to the bottom. I'm going to rename this as final chain as this, and I'll just call this, like, chain or say no history. I need to use this chain here in here and then we're going to use the final chain in our output. If I pop in here now, I want to say that the signature to this is pretty simple, but it's you give it a runnable and that's gonna be our no history. And then you give it a handful of things. The first thing we want to give it is an input message key and that's gonna be, you know, question. That's the what the input is coming in as. And then where you're gonna say maybe we have a history messages key and that's gonna be say chat history. And then we have our answer oh, sorry. Output messages key, and that's gonna be, say, answer. And so what this says is coming in our met our question matter our question is gonna come in as the dictionary key question. Our chat history is gonna come in as the dictionary key chat history, and our output message is and our output from our runnable here without history is gonna come out as here question, and then output message is gonna come out as answer. What this is gonna do then is it's gonna save this somewhere. So we can use Postgres to save this off and how we do that is we say get session history and then we can define a function for this. I'm gonna just call it history retriever history retriever. So if I create this history retriever method here just above, the organization is not great and I just say def history retriever. It's going to take a session ID, a session ID, and then it's going to actually, we can write this as a Lambda. Might be a little simpler. So we'll just say history retriever equals Lambda and we'll take in a session ID and we'll use oh, cool. It's auto completing for me. So we'll use SQL chat message history. What this is, it uses a SQL adapter or SQL alchemy, actually, to go out and find the chat message history. And blank chain handle handles all the migrations and everything else you would expect from this. It takes two arguments, connection string. And so we'll grab both of those things and we'll grab our Postgres and we're gonna say memory URL. If you remember from other videos, our end file had a Postgres memory URL, which is a local memory URL here, and we're gonna use the database history. We could call it whatever we want, PDF, RAG, history. And then we have our session ID. And so the session ID is how it knows who's talking because if we just stored all this in memory or something like that, it'd be impossible or we would be combining conversations. You could leak private information. That would be really bad. What this does is it creates, like, one single string of conversation per session, so it makes it really clean and easy to use. That's it right now. If I ask something of this, we will actually get an error and we can drive that, actually. So what is the case about? So boom, missing key session ID configurable. So that's the first thing. So first things first, our front end needs to send up our session ID So we can do that pretty easily. Let's stop this from reloading by just refreshing here or else it's just gonna keep retrying. So we can have our front end send up a session ID by creating a reference inside of our JavaScript. So if I just say use ref and then make this a string and maybe this defaults to that. Yeah, that's fine. And do I have this? Nope. So we need to install this dependency or maybe it's already installed, but it is going to be this. Yeah, it's already installed. So you're going to install new uidv4, new uidv4, and that gives you this. This is just the default that gets added right away. And then we're gonna use a useEffect method to save away our current session. So or a session ID. We'll say useEffect. UseEffect's run anytime the dependencies change or the component is loaded, and then we can just say session ID ref equals a new ID and let's save this off as session ID ref equals equals that. Cool. So now we need to use that in our request and it actually is telling us exactly where this goes in my request here. I need to pass in config and then you notice configur configur Oh my God. Configurable. And I can say sessionid=sessionid. Current So that gives me the sessionid coming up. So now and if you notice it's passed in that way to the invoke, I can say what is the case about and boom, no database PDF rag history. So let's stop that from running over and over again and let's connect to our database. We can connect to our local database just like this. We don't have to have a database to connect to. We'll just connect to the default Postgres and then we can say create database and then whatever we named it, which I forgot already, we named it PDF right history. And we pop in here and do that. And now our chat is running, and we can say what is the case about. And it looks we're doing okay here. But let's go look at LangSmith. If we look at LangSmith, it's loading our contacts. I reloaded the page. Believe me. And I click on this. We'll see that there is no we have a load history. What is the case about? But we don't actually ever use that history. And if I ask another question, who will tell me more? We get a little bit more here, but it doesn't really know what we're asking. And if we look our rag here, you know, we have the question. This all come in a second. Cool. Tell me more. There is no wait for the output to finish, and then we'll get a little bit better information from here. But if you notice, there's no chat history. There's nothing that has to do with the case, and the prompt that got sent to OpenAI doesn't have anything that has to do with the previous question. And so that's actually a problem. And so what we need to do is actually add to our chain. So we're gonna build it out a little bit more. And Slang Chain has a bit that they have done for us here, but we have to do a lot ourselves too. So I'm gonna paste in a template and I'll talk you through it. But the template's pretty straightforward. If I pop in and I say a template here is given the following conversation, add a follow-up question and follow-up question, rephrase the follow-up question to be a standalone question and it's original in its original language. So we insert our chat history here, we pass in our follow-up question, which would be the question that we are asking, and then the LLM is going to look at all the history. So what's been handed out and then hand back in new a new question that's rephrased to include that. So now we have to actually use this standalone question. To do that, we are going to create a stand alone question, little mini chain. Our little standalone question mini chain is going to return a standalone question out of it and it's gonna be a runnable parallel and in that run, that's going to have a question and that's gonna be a runnable pass through And I'll talk you through why in a second. Oops. And it's going to take a chat history. And so chat history is going to be a Lambda and it's going to just pull out the and actually we can make this a little bit better. We'll say get, buffer string. This will handle if the strings buffered at all. And from there, what we wanna do is pipe this out through our prompt that we just is called the condensed question prompt. And then we wanna pipe that into our LLM, which is gonna be OpenAI, and we'll pipe that into a string output parser. And if I do all of that, I now have a runnable oh, that I I, like, created a class. I was just moving too fast. There we go. And we can run this again and say, what is the the other's name? And we get a key error question. That's a pretty good error to drive from. So when we ask a question, we end up with a questionnaire. This question what's happening here is two things, but I think it's important to understand how our prompts get changed together and what they expect. So when you think of these chains, what we have here is a chain of essentially dictionaries getting passed into each other. So we have, you know, a dictionary that is standalone question that's getting passed into no history. And that's actually the issue because if we go up to our no history chain here, we expect a question. And so if I change this here to question, it should work. If I refresh this, then I say, what is the judge's name? Let's see what we get here. And we get answers. There we go. The judge's name, blah blah blah, residing over what's their first name. And if I look here, what's their first name comes in, and this is now successful, which is what you would and we look in here. We have what is the judge's name. We have our load history. We map this. We say, here's our chat history. What is the case about? And then we pop down and we have our reworded question given the following conversation. So what is the name of the judge presiding over the like this? And then we come into here and we say, what is this? What is their first name and what is the first name of the judge in the Epic versus Apple case? So and then that gets passed into our retriever and we are good to go and we get our answer. That's exactly what I would expect. That's all we needed to do. We use a prompt to create that standalone question that we can pass into the LLM. You also could pass the history to the LLM, or you could use things like thread like threads inside of OpenAI, but this is a lot more generic of a way to do it. We also use a multi query retriever in order to expand the breadth of our vector search and find more relevant documents. We have a front end that accepts streaming. We have a server and then of course the whole thing was deployed DigitalOcean. So let's look one more time at what we did. So we added our multi query retriever here. That's everything you would expect in the multi query. We created a final chain with message history. We added a standalone question and our standalone question prompt and our Lambda that uses SQLAlchemy to go look in our database and get our message history. Then we added to our front end a session ID, and we create that session ID when the component loads and we pass it up into the back end. And if I add multi query and session history, we end up having a complete fully functioning rag. Thank you all for watching this. This has been super fun. I really appreciate all of the views and all of the feedback and all the questions. I hope this shows you how easy it is to add just a few more things into your rag to get a little bit more sophisticated. And so this should be pretty fun and I'll talk to you guys soon. Oh, yeah. Don't forget to like and subscribe.
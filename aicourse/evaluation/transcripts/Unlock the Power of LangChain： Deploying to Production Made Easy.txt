Y'all. Austin Vance here, CEO, cofounder of Focus Labs. I am super excited to start working on part three of the PDF RAG Lang chain demo that I've been doing over the last few weeks. Part three, if you don't remember, is gonna be focused on deploying a RAG to production. So in this video, we will focus on going from our working local RAG, which has a React plus Tailwind front end and a langchain LaSalle back end using Langserve all the way up into the DigitalOcean App Platform I am going to dive right into code Let's get back and remember a little bit of what we did, but I don't want to spend the whole time doing this. If you remember, we started with a loader. The loader loads a set of source documents hiding in this source docs directory. Those documents are loaded using the unstructured PDF loader, embedded using OpenAI's embedding. We're using the most modern OpenAI embedding model. And then we are semantically chunking those things out. And the semantic chunkers are the cool new way to put different sentences together that have semantic meaning. So eventually your rag, when it goes and finds things, can find the most important chunks of a document more simply. Then we then we put all of this into a PG vector store. I love PG vector. It's super fast, super easy, and even better. It's really well supported across all of the cloud providers. I'm going to use DigitalOcean because it's relatively cheap, relatively simple, but it works in Google, works in Amazon, works in Azure. All those are great. We then built out after we got our importer going, we built out our server And if you remember the lang serve servers are super easy All we did is we just added our RAG endpoint with our chain and then we serve all of our static assets, which you can see are our source documents through the exact same server that langchain chain uses our Postgres vector or PG vector store as a vector and then as a retriever in our LCEL or LaSalle pipeline We use, some very simple RAG template that we build out and then we have our entire thing typed so that our lang serve can infer types in its documentation. And then the front end, which was a create react app, ends up being fairly simple too. I put this entire front end into one component. If you don't remember, this one component looks a lot like this. So it's a chat application with simple streaming calling up to the back end using Fetch Event Source, which is a Microsoft library to give you a little bit more robust service and events and we handle streaming data by chunking out by adding chunked messages. We also ask and have our back end oh, I forgot to run the server, so that will probably help if I run this. Let's go here and do playing serve serve here. There we go. And as you can see, as soon as I serve it, the retry logic gets it going and we get our retrieved documents as well as a summary or a LLM style response of what it learned from those documents. So now this doesn't do anybody any good if it's not up in the cloud. So we really want to get it deployed. There's a little bit of work we have to do before we can get it 12 factor deploy ready. The nice thing about DigitalOcean is when we use the app platform, Langserve already comes built in with a if you see here with a Dockerfile. So our Dockerfile lives right here. And so with Langserve coming straight here with our Dockerfile, what we can do is start by adding some dependencies into this Dockerfile that we would need. And if you remember, we use PgVector and our base Python slim debian image doesn't come with the Postgres standard libraries in it. And so let's make sure we get those added and because I've done this before of course Copilot gets to be super helpful with me and so it adds in an apt update and then an apt install and this -yes all it means is don't ask any questions. So, just install the libpqdev This will give us all of the system packages that we would need to run and interact with Postgres inside of our container And then the other thing we need to do is we have our source documents and if you notice the default here, what it does is it installs our poetry, doesn't create virtual environments, creates a working directory, runs poetry install, copies our app over, runs another install, and then we're good to go. So this is awesome. Works great, except for we don't get our source documents. And so what we had done is we had this link here would click and show us the court case documents right away. And if we do this, we need to be able to have our source documents inside of our package or inside of our container with us. If I were doing this, like, boldly for production, I would serve these from a CDN out of s three or using CloudFlare or DigitalOcean CDN. But I think that would get too complex for this demo and the general idea is the same, but I do want to get us running. So after we copy our app and actually, I can put this pretty high up because I don't expect my source documents to change very often. I will copy my source documents. So I would just say something along the lines of copy and then source docs to source docs. And all that's doing is moving our source documents into our source documents directory. So we end up having this exact same structure inside of our Docker container. And that's really all we need to do in in Docker to be ready to deploy. DigitalOcean will then be able to use this Docker image. It'll do it all for us. We don't actually even have to push it up to a registry, which makes it really easy. No other credentials, no other things needed. Instead, we just work straight off of this. So one other thing we need to look at is if you remember in our front end, we hard coded local host and that's really not gonna work for us. If we hard code local host, of course, it's just gonna keep calling local host even from the Internet and that's not gonna get us anywhere. And so we should include a back end URL. So back end URL can come in through the nice thing about the the nice thing about using create react app is it can come in through, environment variables. So I would just do process process. End, and then we can say react app backend URL or something like that. Yeah. Let's call it react app backend Backend URL and if we grab this and we open our our dot env and our front end we have our React App Backend URL here for our local host that was already there for us and then we can add this when we deploy our front end to DigitalOcean. That should be good there. That gives us our React App Backend URL that gives us a Docker file with everything that we need and I think that should be it. Let me check my notes really quickly. I think that should hover it. Let's try and deploy and we can error drive it if we don't have any luck. So when we deploy to DigitalOcean, there's two things we need to do and I'm going to start the database first. Database takes a little longer to provision, makes it harder. If you go through the app first and this is just a kind of a gotcha, there's an option to create a dev database. These dev databases are really small and last time I used one it didn't actually have PgVector installed They might have changed that now, but I just create a dev database or I create a real managed database and that full managed database we can just use Postgres 16 We can use a shared regular disk. You can crank this up for production to anything you want. I just use generally the smallest possible thing. And for this 10 gigabytes is fine. And then we can call it something simple. We'll call it the PDFrag DB, and we'll put it inside our PDFrag demo. And this is gonna cost us $15 a month. So one thing I should note is if you do use DigitalOcean to test this out, I'll include a referral link for DigitalOcean inside of the description. If you use that referral link, you'll get $200 of credit for sixty days. So you won't pay anything to try this stuff out, to run it, to play with it, to deploy it. So use that link. We get a little kickback for our hosting too, but we're not spending very much here either. So, not a huge deal, but it I thought it'd be helpful for everybody to just share that out. So feel free to use that link. So when I create oh, I cannot use underscores for a cluster name So if I use PDF DB RAG and I click create now RAGDB, sorry. It'll create and it'll start to spin up. And if you notice it's all here. And so now let's go and start to create our app. So to create our app, actually, you need to look at our changes. If we look at our changes here, the first thing we did is we have our Dockerfile updated to install libpq. We add our source directories or source documents into our Dockerfile and we replace our backend URL with our, environment variable and that's pretty simple And let's go from here. Let's say get ready to deploy. And I actually am working over a bad history, so I'm going to force push this, but please do not force push your branches unless you know what you're doing. I'm gonna blow away my history, and that should give me everything I need to go. Alright. And we're force updated, blew away some history of me testing this deploy to make sure it would work and go smoothly in the demo for you all. And our RAG database is almost ready. So if I come into the apps portion and so the apps portion of DigitalOcean, Droplets are a think like an EC2 VM. They're just a straight up computer while apps are more like a Heroku style deployment experience where you push code and it just runs like a PaaS or a Platform as a Service And so what we're gonna do is we're gonna pull from GitHub our registry and I can say PDF drag here and I'm gonna pull from the main and I'm gonna just pull from the main branch and from slash and what that's gonna do is it's gonna just gonna look at slash to be our back end. So this is gonna be our back end deployment And what I wanna go in here and do is say next. It's going to look at that directory and start to figure out some stuff. If you notice it's creating some extra stuff for us. I'm not sure what this function it's trying to do is so I'm gonna remove that and I am going to say edit here and this is gonna be called PDFrag backend. So we'll just name that a little better or we can even just name it backend and if you notice it's a web service running Docker on port eight thousand and 80 and we have one HTTP route and I am going to set this route instead to vbackend and one of the things that this will do is it'll remove this route from our paths So it's like an Nginx path replace and what that does is we can not have a CORS issue as we're routing, you know, from our front end to our back end when our front end is deployed into a CDN so that will work just fine. So now if I come back, I can add, let's go in here. I think these names are a little silly. We'll use a basic plan. We have back end. We can run it at five, you know, run it at $10 and would love to change this name here. I don't know if I can though. Not from here. Nope. Cool. Let's add another resource. We're gonna add some more code. We're not going to add a broad database here. We're gonna add GitHub. We're gonna select again, PDF RAG, and now we're gonna do our front end. So our front end, if you remember is deployed here into slash front end. So what I'm gonna do is I'm gonna say only use the source directory front end now to do what we need to do. So we'll start that it's going to scan our repo to say what's in there and we can edit this name and call it a front end. Now, one thing I've noticed with create react app is it thinks that we're gonna run a web service and it's gonna do npm run. I don't want that. I would like it to just run a static site. So when I run a static site, it'll do a compile of the static site and then serve just straight up static assets. And that's way better because two things. One, it's free and two, we're serving straight out of a CDN. And if we look at our route, we serve from slash and we output just as we would expect. So we have our back end and our front end ready to go. One thing we have to remember is as we deploy, this is going to take a little bit longer than a lot of our other tutorials, but that's okay. For our front end, if you look back at how we did our app, we just have a slash here. So we actually don't even need this URL because we're gonna be talking oh, we do because we're gonna be talking to slash backend slash. So we want to go into our front end here and we wanna add this react app backend URL as a, environment variable and we're gonna say backend and that backend will get appended here. So we go to the same domain, but to slash backend and we just click save and then we're gonna look at our backend and we're gonna need to add a few things. So we're gonna have to add where our Postgres database is as well as we're going to have to add our OpenAI API keys and our embedding model. I'm gonna show the API key because I will destroy it before this video goes live and it's easier to just show you what I'm copying and pasting in. So if I grab this whole file, which is really nice, you notice it refers to some local Postgres stuff. We're not gonna use this memory URL yet. We'll do that in the next video, which will allow us to add chat history to our chatbot. So our OpenAI API key, we can add in here and we can add the value here and encrypt will hide that from our we'll hide that from our logs and everything else. And you can click on this and say, it's encrypted on disk and like that. It's still exposed directly to our backend, just as you would expect. You don't have to do anything fancy to read it. And then for our Postgres URL, what I want to do is I want to go down to this database that we had created and grab its connection string Now, before we do that, we're gonna have to do a couple of things or one thing I'm gonna add this database to my I'm gonna add myself to be allowed to talk to this database So that's the first thing I'm doing. And then I'm going to say, I wanna connect from the public internet as DigitalOcean to the DB and I can just grab my connection string here. You can also grab flags, which is a really nice way to connect. So I can grab this flag and I can go over here and I can go straight to my database and I'm going to create a database called rag pg vector, which is exactly what we called our local database. And then the other thing I wanna do is I wanna create a user and I'm gonna create this user here and I'm gonna call it like, he, we'll call it PDF rag. And then we'll say grant all PDF rag on let me pause for a second. Alright. Sorry. I had to look something up. What I would like to do is grant my PDF rag user the ability to do everything it needs to do on the database we just created. What we do is we connect to that database. And if you see, I can do slash c and then just connect to that PDF rag vector. And I can say grant, create, on schema, Grant create on schema public to and what do we name ourselves? PDF rag. And so that'll allow a new user to do what they need to do in order to create or use the database. Now, you can also create the database in here if you want to. I created it through the console. Just fine. Do it either way and you'll be fine. And so if we go now back to the overview, we can come into our PDF rag. The nice thing is if we create the database via the console, it'll give us a nice connection string, but it read it out of there anyway. And I can go here and I can grab this connection string for my PDFrag user to connect to the PDFrag Vectors database. So what I'm gonna do then is I'm gonna hop back in here. I'm gonna paste that whole string in, and then I'm gonna use Postgres URL as my environment variable. Now there's one other gotcha that I just remembered and we can see if we run into it, but we use unstructured to load documents, if you remember, and we build a lot of unstructured stuff to do that. That may and sometimes does run us out of memory in our build server. If so, what we can do is we can move unstructured or we can only install, you know, the PDF parser portion of, unstructured, which is plenty small to build. So if I go here and we look at the last few things, we have an entire PDF rag that'll deploy for $10 a month. We can get that live. It'll be called the Sea Turtle app, which is fun. We'll have a back end and a front end deployed, one to a CDN, and you can see it's a static site for free plus our back end which was just a cheap instance So let's click Deploy and watch this thing go And our deploy will fail because we do not allow our drag here to connect with our app. So we need to go in here and this is called the sea turtle app if you remember. We need to add that sea turtle app to our We need to add that sea turtle app to our allowed connections And now if I come back up to this app maybe we caught it in time We can go to our build logs and watch it go It's building It's installing It looks like we might have gotten our front end through Back end actually looks like it did pretty well too So maybe we don't run into a memory issue which would be phenomenal So we wait. So if you, we do need to make unstructured small, which let's just do anyway. We can open up our pyproject toml here. And if you notice in our unstructured, where is it? Our unstructured here, we have all docs. If you just put in PDF, that'll limit it to only PDF type deployment, and then you're fine to go. I'm gonna just do that anyway as a best practice. If we don't fail, that's great. It has failed on me in the past, and so I don't wanna end up with issues where, it fails over intermittently based on what kind of memory the app is giving us. If we look at our activity now, you can see that everything is deploying. It's just doing its thing. We're watching, you can see that we're continuing to install all of our dependencies, bunch of downloading, and it's just cranking through. Looks like we might make it through unstructured, which is great. And we'll be able to be going. Yep. Taking a snapshot of the files, and this is sometimes where it fails on memory because of just all the stuff unstructured has. Let's see if it can get that, and if not, we will, limit ourselves a little bit. Oh, well, you just wait and we continue to wait. So while we wait, one thing that we can do is we can add our embeddings up to our up into our database in the cloud. So because our database is accessible by essentially anyone, as long as they have the username and password, What I can do is I can connect the PDF rag and the PDF rag, and then grab our connection string again and come here and actually look, sorry. I mean, we should grab our connection string and pop into our importer, load in process, and paste that here. Now, if I paste that here oh, wrong thing. Copy that and paste that here. Then what I can do, now we have our rag here. I can run this and this will run and add our embeddings straight into that database. So we're ready to go when this thing deploys Refresh and see what happened here Yeah, build failed So if we look at why our build failed in our activity, when we go to this deployment, it's an out of memory error and that out of memory error is coming straight from exactly what I said unstructured. We can limit it down to just those files or the other thing that you can do is you can use a poetry build group to only and move unstructured to dev dependency and then not build the dev dependency. Let's try this and see if it works. So let's do git status and let's only add our pyproject toml. So if I click if I do add patch, I don't wanna add this because I don't wanna push that into my repo, but I do wanna push this. So I'm gonna say yes, and I'm gonna say git status, git commit only use unstructured PDFs. And if I push this now, the coolest thing is what we will see here inside of our DigitalOcean deployment is a new build started. We can hop in and just watch that happen. So let's let this run. We'll make sure that it can copy all the files that we would expect. And, while we do that, I don't like that this URL is here. So let's just grab this from the environment. So we can say os get end postgres URL, and we load our dotenv. And then all we'll have to do in the future is update our .env file to do what we need. And if we look at our RAG here or sorry, our semantic chunker, it's just chunking along and, oh, I just spotted a bug. We look here the embedding model we're using. Oh, it is still ADA too, which is great. As long as we continue to use this, the default here is the same, but there's no reason not to use embeddings here too. We make sure we use the same thing, in all places. And front end did exactly what we would expect finished in no time and the back end should finish here pretty quickly as well Look at that, the back end finished, it's deploying, this is running We go to the play logs. I love doing this too. I'll do my own here for that. Ah, no module named config. All right. This is a silly bug. You notice here I put config in outside of a Python module. So let's just drop our module into config here. And then that's what we get to do again, deploy one more time. And if we use dash p, you notice we're updating this to be app config and app config. Yes. And the same deal here. This is coming from app config. Yes. And then we fix that embedding model. Let's not pack our commit. And we made that a environment variable. Let's not pack our commit. And so let's just say you get status. It's just the things we wanted for this fix of the bug that we see in DigitalOcean. Add module that config to a module say get push. And if you notice our back end and our front end were successful at building our build logs look great and our front end actually did deploy, but our back end did not. And we won't get a URL or anything for it yet because we haven't had a fully successful deployment. We have to wait until we get that fully successful deployment to see the front end and the back end come up together, and then we're good to go. And what we can do after we do this is we can use a custom domain if we want to and host this live on the Focused Labs website. Again, feel free to use the DigitalOcean, referral code that I drop into the description and please, I love when everybody comments and asks questions. It's really motivating. It makes me want to keep doing these videos. So please ask questions. And if you're so inclined to keep watching this content as we push it out, we'll keep doing Code with Me demos really similar to this through a bunch of OpenAI and Langchain and Lumindex and a whole bunch of other stuff. So we'll just continue to have fun with writing code together. And please click, click subscribe. Love doing these things and the positive feedback and the validation of people smashing the subscribe button is really nice. This all looks really clean. I wish there was more I could do with you rather than just stalling, but I can maybe I don't really know. So we'll wait for this to finish and then we'll deploy it. What I don't wanna do is push this next commit up. Like, I could we can prep it, but I don't wanna push it, I don't want to push it because it'll potentially restart this deployment, which would just make us wait longer. And I don't want to deal with that. We'll watch our deploy logs for a second now. Oh, and I saw an error. Shit. Shoot. What's our error? Ah, continue to make silly little mistakes So if you notice, I'm guessing what happened is we went in here and our environment variable for our backend database URL does not include Psycho PG. And so we need to make sure that we include PostgreSQL plus Psycho PG, and we do that if remember we do that here in our env, file. So we do plus Psycho PG, and here we do not want to say psychopg two. We just want to say psychopg. We click save. That'll update. We can cancel this, and this deployment will start to run right away, And we continue to wait. I'd love ideas in the comments as well if you have any ideas on what you'd like to see next. I put a little pullout, and we actually got a a few responses, which was really nice. We will after we finish this one, I'll do one more part of this PDF rag where we add chat history, memory, multi query, make this a little bit more robust of a lang chain so that can really see the power of LCEL or LaSalle. And then I'm gonna dive into the new agent graphs. I really love the way agents work, the ability to work with tools, to treat a retriever as a tool, to really just dive in. There's some nuance that is really important to start to play with as we deal with those agent graphs. And so that'll be the next set of content we put out is building applications and toying with these agent graphs, which are just an incredible way to work with Langchain and OpenAI or any LLM for that matter. We also had a request to do a LLM local and so running a open source LLM versus using the OpenAI API. We'll do that one as well, probably just in the middle. So before we dive into the agent graphs, what I'll do is maybe modify this PDF rag to start to use a local LLM, and you can use use that as a jumping off point to start diving into free LLMs or ones that, you don't have to pay for, which is great. You know, save some pocket money. Builds will go faster as soon as we get one through because it'll start to cache everything. Doesn't save the caches, of course, if the builds fail. And so there's our app running. And what we should see is a health check pass rather than it saying waiting. Worries me that it says waiting. If I go here ah, green. Great. Just took a second and health check is unavailable. That's surprising. We go here, we look and we have a full URL. Put my URL in there. It is sea turtle app on digital ocean. And hopefully we can ask a question. Let's open up our console and cross our fingers. What is the case about? And look at that. And it looks like our context did not get loaded Did this finish? It did not finish So, it's again because we didn't include Psycho PG in our loading. So let's rerun this load in process. And now because we're using our environment variable and we look at our environment variable oops, wrong. And we look at our environment variable here, what we wanna do is grab and do grab our settings, environment variables on our back end, grab this one and go here and paste in our Postgres URL. This takes a minute to run. I will pause and when it's done, we can try again. Alright. We're back and the process finished. So let's see if this works. So let's come in here and we will say what is the case about? Look at that. Boom. Retrieved our documents and we get a nice summary or LLM kind of conversation of those documents that we retrieved. This is deployed just as we would expect to DigitalOcean really quickly. Let's over, let's go over what we did going back and remembering what we did here. And actually this is a fun one because we can look at our logs. So if I go to our logs and I say git log, we can see that we moved our database. Let's go all the way to the beginning. So here we got ourselves ready to deploy. So what we did is we updated our Dockerfile, put our source into the Dockerfile and had our local host, or we moved our backend URL into a build variable for React. Then after that, we where we only used unstructured. Finally, we added config to a module because it was out of a module and screwing us up. And then we finally moved our database URL to a variable. That leaves us with a complete fully deployed langchain application running with the DigitalOcean app services. The like always, please subscribe, leave comments. I absolutely love everybody here giving a ton of feedback. And I'd also love to just say thank you so much for watching the videos. Thank you so much for everything. And in the next video, what I'd like to do is get straight into the, the rag and dive into things like how do we do multi query to get better search results? How do we do things like chat history? And then maybe if we have some time, if the video doesn't get too long, we end up doing something like citations as well. Hope everybody has a great week, and I will post something again next week. Thank you.